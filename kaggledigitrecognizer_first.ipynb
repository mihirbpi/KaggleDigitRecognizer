{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5bb35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e17815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28aee9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 2\n",
    "DATA_PATH = \"data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "INPUT_SIZE = 28*28\n",
    "HIDDEN_SIZE = 500\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bc2acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.raw_csv = pd.read_csv(csv_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = torch.tensor(self.raw_csv.iloc[index, 1:]).view(28,28).to(torch.float32)\n",
    "        label = F.one_hot(torch.tensor(self.raw_csv.iloc[index, 0]), num_classes=NUM_CLASSES).to(torch.float32)\n",
    "        return (image, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_csv)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aa2cfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.l1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(self.hidden_size, self.num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out  = x.view(-1, self.input_size)\n",
    "        out = self.l1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da7c6ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = DigitsDataset(TRAIN_PATH)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "samples, labels = next(train_iter)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3, i+1)\n",
    "    plt.imshow(samples[i], cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "177ed757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/420], Loss: 32.7742\n",
      "Epoch [1/2], Step [2/420], Loss: 28.4776\n",
      "Epoch [1/2], Step [3/420], Loss: 28.9416\n",
      "Epoch [1/2], Step [4/420], Loss: 33.4897\n",
      "Epoch [1/2], Step [5/420], Loss: 23.9124\n",
      "Epoch [1/2], Step [6/420], Loss: 20.7215\n",
      "Epoch [1/2], Step [7/420], Loss: 8.6540\n",
      "Epoch [1/2], Step [8/420], Loss: 7.3021\n",
      "Epoch [1/2], Step [9/420], Loss: 10.9161\n",
      "Epoch [1/2], Step [10/420], Loss: 6.8304\n",
      "Epoch [1/2], Step [11/420], Loss: 4.9559\n",
      "Epoch [1/2], Step [12/420], Loss: 2.5240\n",
      "Epoch [1/2], Step [13/420], Loss: 3.9556\n",
      "Epoch [1/2], Step [14/420], Loss: 4.8192\n",
      "Epoch [1/2], Step [15/420], Loss: 6.1174\n",
      "Epoch [1/2], Step [16/420], Loss: 3.5671\n",
      "Epoch [1/2], Step [17/420], Loss: 3.1219\n",
      "Epoch [1/2], Step [18/420], Loss: 4.8178\n",
      "Epoch [1/2], Step [19/420], Loss: 5.7964\n",
      "Epoch [1/2], Step [20/420], Loss: 2.8923\n",
      "Epoch [1/2], Step [21/420], Loss: 2.1253\n",
      "Epoch [1/2], Step [22/420], Loss: 0.8880\n",
      "Epoch [1/2], Step [23/420], Loss: 1.9269\n",
      "Epoch [1/2], Step [24/420], Loss: 2.2199\n",
      "Epoch [1/2], Step [25/420], Loss: 2.7396\n",
      "Epoch [1/2], Step [26/420], Loss: 3.2904\n",
      "Epoch [1/2], Step [27/420], Loss: 4.0186\n",
      "Epoch [1/2], Step [28/420], Loss: 2.1119\n",
      "Epoch [1/2], Step [29/420], Loss: 3.0565\n",
      "Epoch [1/2], Step [30/420], Loss: 2.9937\n",
      "Epoch [1/2], Step [31/420], Loss: 3.2567\n",
      "Epoch [1/2], Step [32/420], Loss: 3.4672\n",
      "Epoch [1/2], Step [33/420], Loss: 0.7709\n",
      "Epoch [1/2], Step [34/420], Loss: 2.2790\n",
      "Epoch [1/2], Step [35/420], Loss: 2.7601\n",
      "Epoch [1/2], Step [36/420], Loss: 1.5081\n",
      "Epoch [1/2], Step [37/420], Loss: 1.5520\n",
      "Epoch [1/2], Step [38/420], Loss: 1.8145\n",
      "Epoch [1/2], Step [39/420], Loss: 1.9821\n",
      "Epoch [1/2], Step [40/420], Loss: 1.6597\n",
      "Epoch [1/2], Step [41/420], Loss: 2.7687\n",
      "Epoch [1/2], Step [42/420], Loss: 0.6023\n",
      "Epoch [1/2], Step [43/420], Loss: 3.7435\n",
      "Epoch [1/2], Step [44/420], Loss: 1.6879\n",
      "Epoch [1/2], Step [45/420], Loss: 2.3533\n",
      "Epoch [1/2], Step [46/420], Loss: 1.2547\n",
      "Epoch [1/2], Step [47/420], Loss: 0.9713\n",
      "Epoch [1/2], Step [48/420], Loss: 2.2092\n",
      "Epoch [1/2], Step [49/420], Loss: 0.4931\n",
      "Epoch [1/2], Step [50/420], Loss: 1.6150\n",
      "Epoch [1/2], Step [51/420], Loss: 1.5871\n",
      "Epoch [1/2], Step [52/420], Loss: 0.6868\n",
      "Epoch [1/2], Step [53/420], Loss: 0.9266\n",
      "Epoch [1/2], Step [54/420], Loss: 0.6785\n",
      "Epoch [1/2], Step [55/420], Loss: 0.4879\n",
      "Epoch [1/2], Step [56/420], Loss: 1.0712\n",
      "Epoch [1/2], Step [57/420], Loss: 1.7422\n",
      "Epoch [1/2], Step [58/420], Loss: 0.7079\n",
      "Epoch [1/2], Step [59/420], Loss: 0.8858\n",
      "Epoch [1/2], Step [60/420], Loss: 0.6245\n",
      "Epoch [1/2], Step [61/420], Loss: 0.4540\n",
      "Epoch [1/2], Step [62/420], Loss: 1.2675\n",
      "Epoch [1/2], Step [63/420], Loss: 0.9103\n",
      "Epoch [1/2], Step [64/420], Loss: 1.1106\n",
      "Epoch [1/2], Step [65/420], Loss: 0.8059\n",
      "Epoch [1/2], Step [66/420], Loss: 0.3487\n",
      "Epoch [1/2], Step [67/420], Loss: 1.2506\n",
      "Epoch [1/2], Step [68/420], Loss: 1.2322\n",
      "Epoch [1/2], Step [69/420], Loss: 1.3886\n",
      "Epoch [1/2], Step [70/420], Loss: 1.6050\n",
      "Epoch [1/2], Step [71/420], Loss: 1.3942\n",
      "Epoch [1/2], Step [72/420], Loss: 0.1908\n",
      "Epoch [1/2], Step [73/420], Loss: 0.7315\n",
      "Epoch [1/2], Step [74/420], Loss: 0.7012\n",
      "Epoch [1/2], Step [75/420], Loss: 0.2867\n",
      "Epoch [1/2], Step [76/420], Loss: 1.0110\n",
      "Epoch [1/2], Step [77/420], Loss: 0.6281\n",
      "Epoch [1/2], Step [78/420], Loss: 1.3898\n",
      "Epoch [1/2], Step [79/420], Loss: 1.4850\n",
      "Epoch [1/2], Step [80/420], Loss: 0.7467\n",
      "Epoch [1/2], Step [81/420], Loss: 1.0071\n",
      "Epoch [1/2], Step [82/420], Loss: 0.5437\n",
      "Epoch [1/2], Step [83/420], Loss: 1.1332\n",
      "Epoch [1/2], Step [84/420], Loss: 0.6119\n",
      "Epoch [1/2], Step [85/420], Loss: 0.4804\n",
      "Epoch [1/2], Step [86/420], Loss: 0.5726\n",
      "Epoch [1/2], Step [87/420], Loss: 0.1947\n",
      "Epoch [1/2], Step [88/420], Loss: 0.5496\n",
      "Epoch [1/2], Step [89/420], Loss: 0.8434\n",
      "Epoch [1/2], Step [90/420], Loss: 0.5633\n",
      "Epoch [1/2], Step [91/420], Loss: 0.4772\n",
      "Epoch [1/2], Step [92/420], Loss: 0.2941\n",
      "Epoch [1/2], Step [93/420], Loss: 0.6095\n",
      "Epoch [1/2], Step [94/420], Loss: 0.6674\n",
      "Epoch [1/2], Step [95/420], Loss: 1.3511\n",
      "Epoch [1/2], Step [96/420], Loss: 0.4689\n",
      "Epoch [1/2], Step [97/420], Loss: 1.1029\n",
      "Epoch [1/2], Step [98/420], Loss: 0.8791\n",
      "Epoch [1/2], Step [99/420], Loss: 0.8743\n",
      "Epoch [1/2], Step [100/420], Loss: 0.9323\n",
      "Epoch [1/2], Step [101/420], Loss: 0.2826\n",
      "Epoch [1/2], Step [102/420], Loss: 0.4814\n",
      "Epoch [1/2], Step [103/420], Loss: 0.5981\n",
      "Epoch [1/2], Step [104/420], Loss: 0.6323\n",
      "Epoch [1/2], Step [105/420], Loss: 0.1645\n",
      "Epoch [1/2], Step [106/420], Loss: 1.3522\n",
      "Epoch [1/2], Step [107/420], Loss: 0.4713\n",
      "Epoch [1/2], Step [108/420], Loss: 0.4352\n",
      "Epoch [1/2], Step [109/420], Loss: 0.9788\n",
      "Epoch [1/2], Step [110/420], Loss: 1.0154\n",
      "Epoch [1/2], Step [111/420], Loss: 0.6528\n",
      "Epoch [1/2], Step [112/420], Loss: 0.7569\n",
      "Epoch [1/2], Step [113/420], Loss: 0.7529\n",
      "Epoch [1/2], Step [114/420], Loss: 0.2408\n",
      "Epoch [1/2], Step [115/420], Loss: 0.6753\n",
      "Epoch [1/2], Step [116/420], Loss: 0.5918\n",
      "Epoch [1/2], Step [117/420], Loss: 0.9717\n",
      "Epoch [1/2], Step [118/420], Loss: 0.5257\n",
      "Epoch [1/2], Step [119/420], Loss: 0.4724\n",
      "Epoch [1/2], Step [120/420], Loss: 0.5163\n",
      "Epoch [1/2], Step [121/420], Loss: 0.2671\n",
      "Epoch [1/2], Step [122/420], Loss: 0.4847\n",
      "Epoch [1/2], Step [123/420], Loss: 1.0993\n",
      "Epoch [1/2], Step [124/420], Loss: 0.6136\n",
      "Epoch [1/2], Step [125/420], Loss: 0.5591\n",
      "Epoch [1/2], Step [126/420], Loss: 0.4578\n",
      "Epoch [1/2], Step [127/420], Loss: 0.3323\n",
      "Epoch [1/2], Step [128/420], Loss: 0.7895\n",
      "Epoch [1/2], Step [129/420], Loss: 0.3905\n",
      "Epoch [1/2], Step [130/420], Loss: 0.5656\n",
      "Epoch [1/2], Step [131/420], Loss: 0.1982\n",
      "Epoch [1/2], Step [132/420], Loss: 0.3062\n",
      "Epoch [1/2], Step [133/420], Loss: 0.5223\n",
      "Epoch [1/2], Step [134/420], Loss: 0.5029\n",
      "Epoch [1/2], Step [135/420], Loss: 0.1172\n",
      "Epoch [1/2], Step [136/420], Loss: 0.5366\n",
      "Epoch [1/2], Step [137/420], Loss: 0.6520\n",
      "Epoch [1/2], Step [138/420], Loss: 0.1017\n",
      "Epoch [1/2], Step [139/420], Loss: 0.0914\n",
      "Epoch [1/2], Step [140/420], Loss: 0.6278\n",
      "Epoch [1/2], Step [141/420], Loss: 0.5114\n",
      "Epoch [1/2], Step [142/420], Loss: 0.4927\n",
      "Epoch [1/2], Step [143/420], Loss: 0.2607\n",
      "Epoch [1/2], Step [144/420], Loss: 0.5426\n",
      "Epoch [1/2], Step [145/420], Loss: 0.1283\n",
      "Epoch [1/2], Step [146/420], Loss: 0.3328\n",
      "Epoch [1/2], Step [147/420], Loss: 0.4725\n",
      "Epoch [1/2], Step [148/420], Loss: 0.3207\n",
      "Epoch [1/2], Step [149/420], Loss: 0.0764\n",
      "Epoch [1/2], Step [150/420], Loss: 1.0281\n",
      "Epoch [1/2], Step [151/420], Loss: 0.5935\n",
      "Epoch [1/2], Step [152/420], Loss: 0.6278\n",
      "Epoch [1/2], Step [153/420], Loss: 0.4304\n",
      "Epoch [1/2], Step [154/420], Loss: 0.2916\n",
      "Epoch [1/2], Step [155/420], Loss: 0.3705\n",
      "Epoch [1/2], Step [156/420], Loss: 0.5095\n",
      "Epoch [1/2], Step [157/420], Loss: 0.4199\n",
      "Epoch [1/2], Step [158/420], Loss: 0.6681\n",
      "Epoch [1/2], Step [159/420], Loss: 0.2118\n",
      "Epoch [1/2], Step [160/420], Loss: 0.3126\n",
      "Epoch [1/2], Step [161/420], Loss: 0.1607\n",
      "Epoch [1/2], Step [162/420], Loss: 0.6040\n",
      "Epoch [1/2], Step [163/420], Loss: 0.6125\n",
      "Epoch [1/2], Step [164/420], Loss: 0.6257\n",
      "Epoch [1/2], Step [165/420], Loss: 0.3646\n",
      "Epoch [1/2], Step [166/420], Loss: 0.1429\n",
      "Epoch [1/2], Step [167/420], Loss: 0.1890\n",
      "Epoch [1/2], Step [168/420], Loss: 0.1047\n",
      "Epoch [1/2], Step [169/420], Loss: 0.4128\n",
      "Epoch [1/2], Step [170/420], Loss: 0.4496\n",
      "Epoch [1/2], Step [171/420], Loss: 0.5741\n",
      "Epoch [1/2], Step [172/420], Loss: 0.3225\n",
      "Epoch [1/2], Step [173/420], Loss: 0.2695\n",
      "Epoch [1/2], Step [174/420], Loss: 0.5693\n",
      "Epoch [1/2], Step [175/420], Loss: 0.2032\n",
      "Epoch [1/2], Step [176/420], Loss: 0.3622\n",
      "Epoch [1/2], Step [177/420], Loss: 0.2223\n",
      "Epoch [1/2], Step [178/420], Loss: 0.3117\n",
      "Epoch [1/2], Step [179/420], Loss: 0.6212\n",
      "Epoch [1/2], Step [180/420], Loss: 0.3097\n",
      "Epoch [1/2], Step [181/420], Loss: 0.3930\n",
      "Epoch [1/2], Step [182/420], Loss: 0.3687\n",
      "Epoch [1/2], Step [183/420], Loss: 0.1103\n",
      "Epoch [1/2], Step [184/420], Loss: 0.5323\n",
      "Epoch [1/2], Step [185/420], Loss: 0.3087\n",
      "Epoch [1/2], Step [186/420], Loss: 0.3435\n",
      "Epoch [1/2], Step [187/420], Loss: 0.2087\n",
      "Epoch [1/2], Step [188/420], Loss: 0.1237\n",
      "Epoch [1/2], Step [189/420], Loss: 0.2438\n",
      "Epoch [1/2], Step [190/420], Loss: 0.4025\n",
      "Epoch [1/2], Step [191/420], Loss: 0.2070\n",
      "Epoch [1/2], Step [192/420], Loss: 0.4384\n",
      "Epoch [1/2], Step [193/420], Loss: 0.3298\n",
      "Epoch [1/2], Step [194/420], Loss: 0.2248\n",
      "Epoch [1/2], Step [195/420], Loss: 0.2054\n",
      "Epoch [1/2], Step [196/420], Loss: 0.3204\n",
      "Epoch [1/2], Step [197/420], Loss: 0.2235\n",
      "Epoch [1/2], Step [198/420], Loss: 0.5654\n",
      "Epoch [1/2], Step [199/420], Loss: 0.2271\n",
      "Epoch [1/2], Step [200/420], Loss: 0.3108\n",
      "Epoch [1/2], Step [201/420], Loss: 0.4034\n",
      "Epoch [1/2], Step [202/420], Loss: 0.1896\n",
      "Epoch [1/2], Step [203/420], Loss: 0.3963\n",
      "Epoch [1/2], Step [204/420], Loss: 0.2817\n",
      "Epoch [1/2], Step [205/420], Loss: 0.4377\n",
      "Epoch [1/2], Step [206/420], Loss: 0.3540\n",
      "Epoch [1/2], Step [207/420], Loss: 0.3595\n",
      "Epoch [1/2], Step [208/420], Loss: 0.3597\n",
      "Epoch [1/2], Step [209/420], Loss: 0.3979\n",
      "Epoch [1/2], Step [210/420], Loss: 0.3108\n",
      "Epoch [1/2], Step [211/420], Loss: 0.3420\n",
      "Epoch [1/2], Step [212/420], Loss: 0.2208\n",
      "Epoch [1/2], Step [213/420], Loss: 0.1941\n",
      "Epoch [1/2], Step [214/420], Loss: 0.2426\n",
      "Epoch [1/2], Step [215/420], Loss: 0.2973\n",
      "Epoch [1/2], Step [216/420], Loss: 0.3786\n",
      "Epoch [1/2], Step [217/420], Loss: 0.0720\n",
      "Epoch [1/2], Step [218/420], Loss: 0.1719\n",
      "Epoch [1/2], Step [219/420], Loss: 0.1661\n",
      "Epoch [1/2], Step [220/420], Loss: 0.3537\n",
      "Epoch [1/2], Step [221/420], Loss: 0.1562\n",
      "Epoch [1/2], Step [222/420], Loss: 0.2446\n",
      "Epoch [1/2], Step [223/420], Loss: 0.4693\n",
      "Epoch [1/2], Step [224/420], Loss: 0.2863\n",
      "Epoch [1/2], Step [225/420], Loss: 0.4716\n",
      "Epoch [1/2], Step [226/420], Loss: 0.1614\n",
      "Epoch [1/2], Step [227/420], Loss: 0.1934\n",
      "Epoch [1/2], Step [228/420], Loss: 0.1001\n",
      "Epoch [1/2], Step [229/420], Loss: 0.3232\n",
      "Epoch [1/2], Step [230/420], Loss: 0.0532\n",
      "Epoch [1/2], Step [231/420], Loss: 0.2965\n",
      "Epoch [1/2], Step [232/420], Loss: 0.1251\n",
      "Epoch [1/2], Step [233/420], Loss: 0.2497\n",
      "Epoch [1/2], Step [234/420], Loss: 0.1251\n",
      "Epoch [1/2], Step [235/420], Loss: 0.4076\n",
      "Epoch [1/2], Step [236/420], Loss: 0.3301\n",
      "Epoch [1/2], Step [237/420], Loss: 0.3963\n",
      "Epoch [1/2], Step [238/420], Loss: 0.3503\n",
      "Epoch [1/2], Step [239/420], Loss: 0.3215\n",
      "Epoch [1/2], Step [240/420], Loss: 0.2917\n",
      "Epoch [1/2], Step [241/420], Loss: 0.3162\n",
      "Epoch [1/2], Step [242/420], Loss: 0.3835\n",
      "Epoch [1/2], Step [243/420], Loss: 0.5582\n",
      "Epoch [1/2], Step [244/420], Loss: 0.3552\n",
      "Epoch [1/2], Step [245/420], Loss: 0.3930\n",
      "Epoch [1/2], Step [246/420], Loss: 0.3605\n",
      "Epoch [1/2], Step [247/420], Loss: 0.1993\n",
      "Epoch [1/2], Step [248/420], Loss: 0.3268\n",
      "Epoch [1/2], Step [249/420], Loss: 0.4681\n",
      "Epoch [1/2], Step [250/420], Loss: 0.1741\n",
      "Epoch [1/2], Step [251/420], Loss: 0.1379\n",
      "Epoch [1/2], Step [252/420], Loss: 0.0631\n",
      "Epoch [1/2], Step [253/420], Loss: 0.2074\n",
      "Epoch [1/2], Step [254/420], Loss: 0.2172\n",
      "Epoch [1/2], Step [255/420], Loss: 0.1590\n",
      "Epoch [1/2], Step [256/420], Loss: 0.1885\n",
      "Epoch [1/2], Step [257/420], Loss: 0.1561\n",
      "Epoch [1/2], Step [258/420], Loss: 0.3144\n",
      "Epoch [1/2], Step [259/420], Loss: 0.2540\n",
      "Epoch [1/2], Step [260/420], Loss: 0.2825\n",
      "Epoch [1/2], Step [261/420], Loss: 0.2761\n",
      "Epoch [1/2], Step [262/420], Loss: 0.6009\n",
      "Epoch [1/2], Step [263/420], Loss: 0.2003\n",
      "Epoch [1/2], Step [264/420], Loss: 0.3765\n",
      "Epoch [1/2], Step [265/420], Loss: 0.2010\n",
      "Epoch [1/2], Step [266/420], Loss: 0.1156\n",
      "Epoch [1/2], Step [267/420], Loss: 0.1645\n",
      "Epoch [1/2], Step [268/420], Loss: 0.5463\n",
      "Epoch [1/2], Step [269/420], Loss: 0.2155\n",
      "Epoch [1/2], Step [270/420], Loss: 0.1692\n",
      "Epoch [1/2], Step [271/420], Loss: 0.1095\n",
      "Epoch [1/2], Step [272/420], Loss: 0.2830\n",
      "Epoch [1/2], Step [273/420], Loss: 0.1270\n",
      "Epoch [1/2], Step [274/420], Loss: 0.0844\n",
      "Epoch [1/2], Step [275/420], Loss: 0.4553\n",
      "Epoch [1/2], Step [276/420], Loss: 0.1068\n",
      "Epoch [1/2], Step [277/420], Loss: 0.2803\n",
      "Epoch [1/2], Step [278/420], Loss: 0.2522\n",
      "Epoch [1/2], Step [279/420], Loss: 0.1192\n",
      "Epoch [1/2], Step [280/420], Loss: 0.4209\n",
      "Epoch [1/2], Step [281/420], Loss: 0.2243\n",
      "Epoch [1/2], Step [282/420], Loss: 0.5864\n",
      "Epoch [1/2], Step [283/420], Loss: 0.4299\n",
      "Epoch [1/2], Step [284/420], Loss: 0.1728\n",
      "Epoch [1/2], Step [285/420], Loss: 0.4000\n",
      "Epoch [1/2], Step [286/420], Loss: 0.1656\n",
      "Epoch [1/2], Step [287/420], Loss: 0.1433\n",
      "Epoch [1/2], Step [288/420], Loss: 0.2138\n",
      "Epoch [1/2], Step [289/420], Loss: 0.2472\n",
      "Epoch [1/2], Step [290/420], Loss: 0.3448\n",
      "Epoch [1/2], Step [291/420], Loss: 0.3646\n",
      "Epoch [1/2], Step [292/420], Loss: 0.1736\n",
      "Epoch [1/2], Step [293/420], Loss: 0.0560\n",
      "Epoch [1/2], Step [294/420], Loss: 0.1681\n",
      "Epoch [1/2], Step [295/420], Loss: 0.1794\n",
      "Epoch [1/2], Step [296/420], Loss: 0.2436\n",
      "Epoch [1/2], Step [297/420], Loss: 0.1778\n",
      "Epoch [1/2], Step [298/420], Loss: 0.1584\n",
      "Epoch [1/2], Step [299/420], Loss: 0.2308\n",
      "Epoch [1/2], Step [300/420], Loss: 0.4322\n",
      "Epoch [1/2], Step [301/420], Loss: 0.3339\n",
      "Epoch [1/2], Step [302/420], Loss: 0.1921\n",
      "Epoch [1/2], Step [303/420], Loss: 0.1770\n",
      "Epoch [1/2], Step [304/420], Loss: 0.1752\n",
      "Epoch [1/2], Step [305/420], Loss: 0.3250\n",
      "Epoch [1/2], Step [306/420], Loss: 0.2659\n",
      "Epoch [1/2], Step [307/420], Loss: 0.2200\n",
      "Epoch [1/2], Step [308/420], Loss: 0.5327\n",
      "Epoch [1/2], Step [309/420], Loss: 0.1736\n",
      "Epoch [1/2], Step [310/420], Loss: 0.2455\n",
      "Epoch [1/2], Step [311/420], Loss: 0.1997\n",
      "Epoch [1/2], Step [312/420], Loss: 0.2202\n",
      "Epoch [1/2], Step [313/420], Loss: 0.5407\n",
      "Epoch [1/2], Step [314/420], Loss: 0.2192\n",
      "Epoch [1/2], Step [315/420], Loss: 0.3149\n",
      "Epoch [1/2], Step [316/420], Loss: 0.2193\n",
      "Epoch [1/2], Step [317/420], Loss: 0.1664\n",
      "Epoch [1/2], Step [318/420], Loss: 0.2962\n",
      "Epoch [1/2], Step [319/420], Loss: 0.3513\n",
      "Epoch [1/2], Step [320/420], Loss: 0.0797\n",
      "Epoch [1/2], Step [321/420], Loss: 0.2080\n",
      "Epoch [1/2], Step [322/420], Loss: 0.6740\n",
      "Epoch [1/2], Step [323/420], Loss: 0.0944\n",
      "Epoch [1/2], Step [324/420], Loss: 0.3168\n",
      "Epoch [1/2], Step [325/420], Loss: 0.2523\n",
      "Epoch [1/2], Step [326/420], Loss: 0.1990\n",
      "Epoch [1/2], Step [327/420], Loss: 0.4610\n",
      "Epoch [1/2], Step [328/420], Loss: 0.3116\n",
      "Epoch [1/2], Step [329/420], Loss: 0.3334\n",
      "Epoch [1/2], Step [330/420], Loss: 0.2510\n",
      "Epoch [1/2], Step [331/420], Loss: 0.1547\n",
      "Epoch [1/2], Step [332/420], Loss: 0.1164\n",
      "Epoch [1/2], Step [333/420], Loss: 0.1417\n",
      "Epoch [1/2], Step [334/420], Loss: 0.2266\n",
      "Epoch [1/2], Step [335/420], Loss: 0.2400\n",
      "Epoch [1/2], Step [336/420], Loss: 0.3617\n",
      "Epoch [1/2], Step [337/420], Loss: 0.3621\n",
      "Epoch [1/2], Step [338/420], Loss: 0.1221\n",
      "Epoch [1/2], Step [339/420], Loss: 0.4089\n",
      "Epoch [1/2], Step [340/420], Loss: 0.2818\n",
      "Epoch [1/2], Step [341/420], Loss: 0.1010\n",
      "Epoch [1/2], Step [342/420], Loss: 0.1743\n",
      "Epoch [1/2], Step [343/420], Loss: 0.1055\n",
      "Epoch [1/2], Step [344/420], Loss: 0.4516\n",
      "Epoch [1/2], Step [345/420], Loss: 0.3499\n",
      "Epoch [1/2], Step [346/420], Loss: 0.1864\n",
      "Epoch [1/2], Step [347/420], Loss: 0.5342\n",
      "Epoch [1/2], Step [348/420], Loss: 0.3355\n",
      "Epoch [1/2], Step [349/420], Loss: 0.2869\n",
      "Epoch [1/2], Step [350/420], Loss: 0.3139\n",
      "Epoch [1/2], Step [351/420], Loss: 0.1868\n",
      "Epoch [1/2], Step [352/420], Loss: 0.3073\n",
      "Epoch [1/2], Step [353/420], Loss: 0.2392\n",
      "Epoch [1/2], Step [354/420], Loss: 0.0948\n",
      "Epoch [1/2], Step [355/420], Loss: 0.3234\n",
      "Epoch [1/2], Step [356/420], Loss: 0.1077\n",
      "Epoch [1/2], Step [357/420], Loss: 0.2800\n",
      "Epoch [1/2], Step [358/420], Loss: 0.1869\n",
      "Epoch [1/2], Step [359/420], Loss: 0.5338\n",
      "Epoch [1/2], Step [360/420], Loss: 0.1557\n",
      "Epoch [1/2], Step [361/420], Loss: 0.2478\n",
      "Epoch [1/2], Step [362/420], Loss: 0.0902\n",
      "Epoch [1/2], Step [363/420], Loss: 0.1458\n",
      "Epoch [1/2], Step [364/420], Loss: 0.1424\n",
      "Epoch [1/2], Step [365/420], Loss: 0.3542\n",
      "Epoch [1/2], Step [366/420], Loss: 0.2835\n",
      "Epoch [1/2], Step [367/420], Loss: 0.2083\n",
      "Epoch [1/2], Step [368/420], Loss: 0.2708\n",
      "Epoch [1/2], Step [369/420], Loss: 0.2223\n",
      "Epoch [1/2], Step [370/420], Loss: 0.1400\n",
      "Epoch [1/2], Step [371/420], Loss: 0.1470\n",
      "Epoch [1/2], Step [372/420], Loss: 0.1002\n",
      "Epoch [1/2], Step [373/420], Loss: 0.1612\n",
      "Epoch [1/2], Step [374/420], Loss: 0.0456\n",
      "Epoch [1/2], Step [375/420], Loss: 0.3259\n",
      "Epoch [1/2], Step [376/420], Loss: 0.4327\n",
      "Epoch [1/2], Step [377/420], Loss: 0.3795\n",
      "Epoch [1/2], Step [378/420], Loss: 0.4396\n",
      "Epoch [1/2], Step [379/420], Loss: 0.3520\n",
      "Epoch [1/2], Step [380/420], Loss: 0.1781\n",
      "Epoch [1/2], Step [381/420], Loss: 0.2150\n",
      "Epoch [1/2], Step [382/420], Loss: 0.2577\n",
      "Epoch [1/2], Step [383/420], Loss: 0.1978\n",
      "Epoch [1/2], Step [384/420], Loss: 0.2533\n",
      "Epoch [1/2], Step [385/420], Loss: 0.2203\n",
      "Epoch [1/2], Step [386/420], Loss: 0.2728\n",
      "Epoch [1/2], Step [387/420], Loss: 0.1106\n",
      "Epoch [1/2], Step [388/420], Loss: 0.3115\n",
      "Epoch [1/2], Step [389/420], Loss: 0.1905\n",
      "Epoch [1/2], Step [390/420], Loss: 0.2004\n",
      "Epoch [1/2], Step [391/420], Loss: 0.2185\n",
      "Epoch [1/2], Step [392/420], Loss: 0.2651\n",
      "Epoch [1/2], Step [393/420], Loss: 0.2401\n",
      "Epoch [1/2], Step [394/420], Loss: 0.2357\n",
      "Epoch [1/2], Step [395/420], Loss: 0.1181\n",
      "Epoch [1/2], Step [396/420], Loss: 0.0916\n",
      "Epoch [1/2], Step [397/420], Loss: 0.3615\n",
      "Epoch [1/2], Step [398/420], Loss: 0.2078\n",
      "Epoch [1/2], Step [399/420], Loss: 0.3936\n",
      "Epoch [1/2], Step [400/420], Loss: 0.6197\n",
      "Epoch [1/2], Step [401/420], Loss: 0.2941\n",
      "Epoch [1/2], Step [402/420], Loss: 0.4098\n",
      "Epoch [1/2], Step [403/420], Loss: 0.3435\n",
      "Epoch [1/2], Step [404/420], Loss: 0.2690\n",
      "Epoch [1/2], Step [405/420], Loss: 0.0876\n",
      "Epoch [1/2], Step [406/420], Loss: 0.2106\n",
      "Epoch [1/2], Step [407/420], Loss: 0.1431\n",
      "Epoch [1/2], Step [408/420], Loss: 0.1832\n",
      "Epoch [1/2], Step [409/420], Loss: 0.1912\n",
      "Epoch [1/2], Step [410/420], Loss: 0.2487\n",
      "Epoch [1/2], Step [411/420], Loss: 0.5283\n",
      "Epoch [1/2], Step [412/420], Loss: 0.1873\n",
      "Epoch [1/2], Step [413/420], Loss: 0.2439\n",
      "Epoch [1/2], Step [414/420], Loss: 0.0577\n",
      "Epoch [1/2], Step [415/420], Loss: 0.2693\n",
      "Epoch [1/2], Step [416/420], Loss: 0.2364\n",
      "Epoch [1/2], Step [417/420], Loss: 0.2289\n",
      "Epoch [1/2], Step [418/420], Loss: 0.1728\n",
      "Epoch [1/2], Step [419/420], Loss: 0.0737\n",
      "Epoch [1/2], Step [420/420], Loss: 0.1573\n",
      "Epoch [2/2], Step [1/420], Loss: 0.2877\n",
      "Epoch [2/2], Step [2/420], Loss: 0.1811\n",
      "Epoch [2/2], Step [3/420], Loss: 0.0631\n",
      "Epoch [2/2], Step [4/420], Loss: 0.2291\n",
      "Epoch [2/2], Step [5/420], Loss: 0.1383\n",
      "Epoch [2/2], Step [6/420], Loss: 0.1537\n",
      "Epoch [2/2], Step [7/420], Loss: 0.1452\n",
      "Epoch [2/2], Step [8/420], Loss: 0.1055\n",
      "Epoch [2/2], Step [9/420], Loss: 0.1972\n",
      "Epoch [2/2], Step [10/420], Loss: 0.0699\n",
      "Epoch [2/2], Step [11/420], Loss: 0.3462\n",
      "Epoch [2/2], Step [12/420], Loss: 0.0558\n",
      "Epoch [2/2], Step [13/420], Loss: 0.2523\n",
      "Epoch [2/2], Step [14/420], Loss: 0.1110\n",
      "Epoch [2/2], Step [15/420], Loss: 0.0597\n",
      "Epoch [2/2], Step [16/420], Loss: 0.1208\n",
      "Epoch [2/2], Step [17/420], Loss: 0.1394\n",
      "Epoch [2/2], Step [18/420], Loss: 0.1600\n",
      "Epoch [2/2], Step [19/420], Loss: 0.2182\n",
      "Epoch [2/2], Step [20/420], Loss: 0.1181\n",
      "Epoch [2/2], Step [21/420], Loss: 0.0640\n",
      "Epoch [2/2], Step [22/420], Loss: 0.2071\n",
      "Epoch [2/2], Step [23/420], Loss: 0.0448\n",
      "Epoch [2/2], Step [24/420], Loss: 0.0804\n",
      "Epoch [2/2], Step [25/420], Loss: 0.1138\n",
      "Epoch [2/2], Step [26/420], Loss: 0.2531\n",
      "Epoch [2/2], Step [27/420], Loss: 0.0986\n",
      "Epoch [2/2], Step [28/420], Loss: 0.0888\n",
      "Epoch [2/2], Step [29/420], Loss: 0.1916\n",
      "Epoch [2/2], Step [30/420], Loss: 0.1044\n",
      "Epoch [2/2], Step [31/420], Loss: 0.2495\n",
      "Epoch [2/2], Step [32/420], Loss: 0.1200\n",
      "Epoch [2/2], Step [33/420], Loss: 0.2078\n",
      "Epoch [2/2], Step [34/420], Loss: 0.1255\n",
      "Epoch [2/2], Step [35/420], Loss: 0.1957\n",
      "Epoch [2/2], Step [36/420], Loss: 0.1665\n",
      "Epoch [2/2], Step [37/420], Loss: 0.2361\n",
      "Epoch [2/2], Step [38/420], Loss: 0.0851\n",
      "Epoch [2/2], Step [39/420], Loss: 0.1669\n",
      "Epoch [2/2], Step [40/420], Loss: 0.1609\n",
      "Epoch [2/2], Step [41/420], Loss: 0.0665\n",
      "Epoch [2/2], Step [42/420], Loss: 0.1041\n",
      "Epoch [2/2], Step [43/420], Loss: 0.2298\n",
      "Epoch [2/2], Step [44/420], Loss: 0.1371\n",
      "Epoch [2/2], Step [45/420], Loss: 0.1385\n",
      "Epoch [2/2], Step [46/420], Loss: 0.0678\n",
      "Epoch [2/2], Step [47/420], Loss: 0.1286\n",
      "Epoch [2/2], Step [48/420], Loss: 0.1152\n",
      "Epoch [2/2], Step [49/420], Loss: 0.1419\n",
      "Epoch [2/2], Step [50/420], Loss: 0.1525\n",
      "Epoch [2/2], Step [51/420], Loss: 0.0847\n",
      "Epoch [2/2], Step [52/420], Loss: 0.1058\n",
      "Epoch [2/2], Step [53/420], Loss: 0.1281\n",
      "Epoch [2/2], Step [54/420], Loss: 0.0850\n",
      "Epoch [2/2], Step [55/420], Loss: 0.1022\n",
      "Epoch [2/2], Step [56/420], Loss: 0.0531\n",
      "Epoch [2/2], Step [57/420], Loss: 0.1068\n",
      "Epoch [2/2], Step [58/420], Loss: 0.1005\n",
      "Epoch [2/2], Step [59/420], Loss: 0.0890\n",
      "Epoch [2/2], Step [60/420], Loss: 0.0822\n",
      "Epoch [2/2], Step [61/420], Loss: 0.0775\n",
      "Epoch [2/2], Step [62/420], Loss: 0.2269\n",
      "Epoch [2/2], Step [63/420], Loss: 0.1297\n",
      "Epoch [2/2], Step [64/420], Loss: 0.1811\n",
      "Epoch [2/2], Step [65/420], Loss: 0.1390\n",
      "Epoch [2/2], Step [66/420], Loss: 0.2074\n",
      "Epoch [2/2], Step [67/420], Loss: 0.1741\n",
      "Epoch [2/2], Step [68/420], Loss: 0.0940\n",
      "Epoch [2/2], Step [69/420], Loss: 0.0986\n",
      "Epoch [2/2], Step [70/420], Loss: 0.1142\n",
      "Epoch [2/2], Step [71/420], Loss: 0.2175\n",
      "Epoch [2/2], Step [72/420], Loss: 0.0546\n",
      "Epoch [2/2], Step [73/420], Loss: 0.1779\n",
      "Epoch [2/2], Step [74/420], Loss: 0.0748\n",
      "Epoch [2/2], Step [75/420], Loss: 0.2352\n",
      "Epoch [2/2], Step [76/420], Loss: 0.0165\n",
      "Epoch [2/2], Step [77/420], Loss: 0.1799\n",
      "Epoch [2/2], Step [78/420], Loss: 0.1006\n",
      "Epoch [2/2], Step [79/420], Loss: 0.0319\n",
      "Epoch [2/2], Step [80/420], Loss: 0.0538\n",
      "Epoch [2/2], Step [81/420], Loss: 0.0992\n",
      "Epoch [2/2], Step [82/420], Loss: 0.1390\n",
      "Epoch [2/2], Step [83/420], Loss: 0.0384\n",
      "Epoch [2/2], Step [84/420], Loss: 0.1755\n",
      "Epoch [2/2], Step [85/420], Loss: 0.0656\n",
      "Epoch [2/2], Step [86/420], Loss: 0.0857\n",
      "Epoch [2/2], Step [87/420], Loss: 0.1363\n",
      "Epoch [2/2], Step [88/420], Loss: 0.1877\n",
      "Epoch [2/2], Step [89/420], Loss: 0.0905\n",
      "Epoch [2/2], Step [90/420], Loss: 0.1416\n",
      "Epoch [2/2], Step [91/420], Loss: 0.0498\n",
      "Epoch [2/2], Step [92/420], Loss: 0.1389\n",
      "Epoch [2/2], Step [93/420], Loss: 0.2122\n",
      "Epoch [2/2], Step [94/420], Loss: 0.0277\n",
      "Epoch [2/2], Step [95/420], Loss: 0.1222\n",
      "Epoch [2/2], Step [96/420], Loss: 0.0437\n",
      "Epoch [2/2], Step [97/420], Loss: 0.0827\n",
      "Epoch [2/2], Step [98/420], Loss: 0.0218\n",
      "Epoch [2/2], Step [99/420], Loss: 0.0606\n",
      "Epoch [2/2], Step [100/420], Loss: 0.1163\n",
      "Epoch [2/2], Step [101/420], Loss: 0.2506\n",
      "Epoch [2/2], Step [102/420], Loss: 0.1818\n",
      "Epoch [2/2], Step [103/420], Loss: 0.1368\n",
      "Epoch [2/2], Step [104/420], Loss: 0.1253\n",
      "Epoch [2/2], Step [105/420], Loss: 0.0349\n",
      "Epoch [2/2], Step [106/420], Loss: 0.0054\n",
      "Epoch [2/2], Step [107/420], Loss: 0.1431\n",
      "Epoch [2/2], Step [108/420], Loss: 0.1803\n",
      "Epoch [2/2], Step [109/420], Loss: 0.0442\n",
      "Epoch [2/2], Step [110/420], Loss: 0.0274\n",
      "Epoch [2/2], Step [111/420], Loss: 0.2060\n",
      "Epoch [2/2], Step [112/420], Loss: 0.0421\n",
      "Epoch [2/2], Step [113/420], Loss: 0.0693\n",
      "Epoch [2/2], Step [114/420], Loss: 0.0710\n",
      "Epoch [2/2], Step [115/420], Loss: 0.0975\n",
      "Epoch [2/2], Step [116/420], Loss: 0.1677\n",
      "Epoch [2/2], Step [117/420], Loss: 0.2209\n",
      "Epoch [2/2], Step [118/420], Loss: 0.2345\n",
      "Epoch [2/2], Step [119/420], Loss: 0.0925\n",
      "Epoch [2/2], Step [120/420], Loss: 0.0620\n",
      "Epoch [2/2], Step [121/420], Loss: 0.0414\n",
      "Epoch [2/2], Step [122/420], Loss: 0.1396\n",
      "Epoch [2/2], Step [123/420], Loss: 0.0558\n",
      "Epoch [2/2], Step [124/420], Loss: 0.1913\n",
      "Epoch [2/2], Step [125/420], Loss: 0.1272\n",
      "Epoch [2/2], Step [126/420], Loss: 0.3872\n",
      "Epoch [2/2], Step [127/420], Loss: 0.1678\n",
      "Epoch [2/2], Step [128/420], Loss: 0.1982\n",
      "Epoch [2/2], Step [129/420], Loss: 0.0501\n",
      "Epoch [2/2], Step [130/420], Loss: 0.0919\n",
      "Epoch [2/2], Step [131/420], Loss: 0.2156\n",
      "Epoch [2/2], Step [132/420], Loss: 0.0812\n",
      "Epoch [2/2], Step [133/420], Loss: 0.1937\n",
      "Epoch [2/2], Step [134/420], Loss: 0.0277\n",
      "Epoch [2/2], Step [135/420], Loss: 0.1040\n",
      "Epoch [2/2], Step [136/420], Loss: 0.0607\n",
      "Epoch [2/2], Step [137/420], Loss: 0.1654\n",
      "Epoch [2/2], Step [138/420], Loss: 0.0908\n",
      "Epoch [2/2], Step [139/420], Loss: 0.0427\n",
      "Epoch [2/2], Step [140/420], Loss: 0.1255\n",
      "Epoch [2/2], Step [141/420], Loss: 0.1907\n",
      "Epoch [2/2], Step [142/420], Loss: 0.0545\n",
      "Epoch [2/2], Step [143/420], Loss: 0.0364\n",
      "Epoch [2/2], Step [144/420], Loss: 0.1363\n",
      "Epoch [2/2], Step [145/420], Loss: 0.1152\n",
      "Epoch [2/2], Step [146/420], Loss: 0.0503\n",
      "Epoch [2/2], Step [147/420], Loss: 0.2620\n",
      "Epoch [2/2], Step [148/420], Loss: 0.1420\n",
      "Epoch [2/2], Step [149/420], Loss: 0.1176\n",
      "Epoch [2/2], Step [150/420], Loss: 0.1751\n",
      "Epoch [2/2], Step [151/420], Loss: 0.0868\n",
      "Epoch [2/2], Step [152/420], Loss: 0.0919\n",
      "Epoch [2/2], Step [153/420], Loss: 0.2418\n",
      "Epoch [2/2], Step [154/420], Loss: 0.2525\n",
      "Epoch [2/2], Step [155/420], Loss: 0.1293\n",
      "Epoch [2/2], Step [156/420], Loss: 0.1690\n",
      "Epoch [2/2], Step [157/420], Loss: 0.2082\n",
      "Epoch [2/2], Step [158/420], Loss: 0.2458\n",
      "Epoch [2/2], Step [159/420], Loss: 0.2193\n",
      "Epoch [2/2], Step [160/420], Loss: 0.2671\n",
      "Epoch [2/2], Step [161/420], Loss: 0.0936\n",
      "Epoch [2/2], Step [162/420], Loss: 0.1557\n",
      "Epoch [2/2], Step [163/420], Loss: 0.1825\n",
      "Epoch [2/2], Step [164/420], Loss: 0.0847\n",
      "Epoch [2/2], Step [165/420], Loss: 0.0318\n",
      "Epoch [2/2], Step [166/420], Loss: 0.2032\n",
      "Epoch [2/2], Step [167/420], Loss: 0.2121\n",
      "Epoch [2/2], Step [168/420], Loss: 0.1627\n",
      "Epoch [2/2], Step [169/420], Loss: 0.0745\n",
      "Epoch [2/2], Step [170/420], Loss: 0.0694\n",
      "Epoch [2/2], Step [171/420], Loss: 0.0600\n",
      "Epoch [2/2], Step [172/420], Loss: 0.1335\n",
      "Epoch [2/2], Step [173/420], Loss: 0.0441\n",
      "Epoch [2/2], Step [174/420], Loss: 0.1881\n",
      "Epoch [2/2], Step [175/420], Loss: 0.1989\n",
      "Epoch [2/2], Step [176/420], Loss: 0.0684\n",
      "Epoch [2/2], Step [177/420], Loss: 0.1633\n",
      "Epoch [2/2], Step [178/420], Loss: 0.1385\n",
      "Epoch [2/2], Step [179/420], Loss: 0.2135\n",
      "Epoch [2/2], Step [180/420], Loss: 0.1124\n",
      "Epoch [2/2], Step [181/420], Loss: 0.1071\n",
      "Epoch [2/2], Step [182/420], Loss: 0.1307\n",
      "Epoch [2/2], Step [183/420], Loss: 0.0319\n",
      "Epoch [2/2], Step [184/420], Loss: 0.1745\n",
      "Epoch [2/2], Step [185/420], Loss: 0.0787\n",
      "Epoch [2/2], Step [186/420], Loss: 0.2255\n",
      "Epoch [2/2], Step [187/420], Loss: 0.2120\n",
      "Epoch [2/2], Step [188/420], Loss: 0.1834\n",
      "Epoch [2/2], Step [189/420], Loss: 0.0541\n",
      "Epoch [2/2], Step [190/420], Loss: 0.0332\n",
      "Epoch [2/2], Step [191/420], Loss: 0.0437\n",
      "Epoch [2/2], Step [192/420], Loss: 0.2695\n",
      "Epoch [2/2], Step [193/420], Loss: 0.1471\n",
      "Epoch [2/2], Step [194/420], Loss: 0.4524\n",
      "Epoch [2/2], Step [195/420], Loss: 0.2109\n",
      "Epoch [2/2], Step [196/420], Loss: 0.1959\n",
      "Epoch [2/2], Step [197/420], Loss: 0.0959\n",
      "Epoch [2/2], Step [198/420], Loss: 0.0694\n",
      "Epoch [2/2], Step [199/420], Loss: 0.0543\n",
      "Epoch [2/2], Step [200/420], Loss: 0.1085\n",
      "Epoch [2/2], Step [201/420], Loss: 0.1555\n",
      "Epoch [2/2], Step [202/420], Loss: 0.3009\n",
      "Epoch [2/2], Step [203/420], Loss: 0.1905\n",
      "Epoch [2/2], Step [204/420], Loss: 0.0644\n",
      "Epoch [2/2], Step [205/420], Loss: 0.0771\n",
      "Epoch [2/2], Step [206/420], Loss: 0.2538\n",
      "Epoch [2/2], Step [207/420], Loss: 0.0817\n",
      "Epoch [2/2], Step [208/420], Loss: 0.0353\n",
      "Epoch [2/2], Step [209/420], Loss: 0.0181\n",
      "Epoch [2/2], Step [210/420], Loss: 0.1554\n",
      "Epoch [2/2], Step [211/420], Loss: 0.2487\n",
      "Epoch [2/2], Step [212/420], Loss: 0.2480\n",
      "Epoch [2/2], Step [213/420], Loss: 0.2352\n",
      "Epoch [2/2], Step [214/420], Loss: 0.0630\n",
      "Epoch [2/2], Step [215/420], Loss: 0.2365\n",
      "Epoch [2/2], Step [216/420], Loss: 0.1138\n",
      "Epoch [2/2], Step [217/420], Loss: 0.1806\n",
      "Epoch [2/2], Step [218/420], Loss: 0.1069\n",
      "Epoch [2/2], Step [219/420], Loss: 0.1664\n",
      "Epoch [2/2], Step [220/420], Loss: 0.0550\n",
      "Epoch [2/2], Step [221/420], Loss: 0.1722\n",
      "Epoch [2/2], Step [222/420], Loss: 0.0256\n",
      "Epoch [2/2], Step [223/420], Loss: 0.2213\n",
      "Epoch [2/2], Step [224/420], Loss: 0.0371\n",
      "Epoch [2/2], Step [225/420], Loss: 0.0351\n",
      "Epoch [2/2], Step [226/420], Loss: 0.3005\n",
      "Epoch [2/2], Step [227/420], Loss: 0.1079\n",
      "Epoch [2/2], Step [228/420], Loss: 0.0639\n",
      "Epoch [2/2], Step [229/420], Loss: 0.0925\n",
      "Epoch [2/2], Step [230/420], Loss: 0.2877\n",
      "Epoch [2/2], Step [231/420], Loss: 0.2278\n",
      "Epoch [2/2], Step [232/420], Loss: 0.2387\n",
      "Epoch [2/2], Step [233/420], Loss: 0.2664\n",
      "Epoch [2/2], Step [234/420], Loss: 0.2005\n",
      "Epoch [2/2], Step [235/420], Loss: 0.2148\n",
      "Epoch [2/2], Step [236/420], Loss: 0.2028\n",
      "Epoch [2/2], Step [237/420], Loss: 0.3176\n",
      "Epoch [2/2], Step [238/420], Loss: 0.0371\n",
      "Epoch [2/2], Step [239/420], Loss: 0.1781\n",
      "Epoch [2/2], Step [240/420], Loss: 0.0960\n",
      "Epoch [2/2], Step [241/420], Loss: 0.1701\n",
      "Epoch [2/2], Step [242/420], Loss: 0.4488\n",
      "Epoch [2/2], Step [243/420], Loss: 0.1962\n",
      "Epoch [2/2], Step [244/420], Loss: 0.2102\n",
      "Epoch [2/2], Step [245/420], Loss: 0.2117\n",
      "Epoch [2/2], Step [246/420], Loss: 0.0792\n",
      "Epoch [2/2], Step [247/420], Loss: 0.2048\n",
      "Epoch [2/2], Step [248/420], Loss: 0.1405\n",
      "Epoch [2/2], Step [249/420], Loss: 0.2968\n",
      "Epoch [2/2], Step [250/420], Loss: 0.1227\n",
      "Epoch [2/2], Step [251/420], Loss: 0.1622\n",
      "Epoch [2/2], Step [252/420], Loss: 0.0694\n",
      "Epoch [2/2], Step [253/420], Loss: 0.0837\n",
      "Epoch [2/2], Step [254/420], Loss: 0.1109\n",
      "Epoch [2/2], Step [255/420], Loss: 0.0852\n",
      "Epoch [2/2], Step [256/420], Loss: 0.0770\n",
      "Epoch [2/2], Step [257/420], Loss: 0.1306\n",
      "Epoch [2/2], Step [258/420], Loss: 0.2257\n",
      "Epoch [2/2], Step [259/420], Loss: 0.2137\n",
      "Epoch [2/2], Step [260/420], Loss: 0.2101\n",
      "Epoch [2/2], Step [261/420], Loss: 0.3022\n",
      "Epoch [2/2], Step [262/420], Loss: 0.1510\n",
      "Epoch [2/2], Step [263/420], Loss: 0.0834\n",
      "Epoch [2/2], Step [264/420], Loss: 0.3394\n",
      "Epoch [2/2], Step [265/420], Loss: 0.0418\n",
      "Epoch [2/2], Step [266/420], Loss: 0.2154\n",
      "Epoch [2/2], Step [267/420], Loss: 0.0765\n",
      "Epoch [2/2], Step [268/420], Loss: 0.1428\n",
      "Epoch [2/2], Step [269/420], Loss: 0.0601\n",
      "Epoch [2/2], Step [270/420], Loss: 0.0491\n",
      "Epoch [2/2], Step [271/420], Loss: 0.1797\n",
      "Epoch [2/2], Step [272/420], Loss: 0.0937\n",
      "Epoch [2/2], Step [273/420], Loss: 0.3283\n",
      "Epoch [2/2], Step [274/420], Loss: 0.2819\n",
      "Epoch [2/2], Step [275/420], Loss: 0.0970\n",
      "Epoch [2/2], Step [276/420], Loss: 0.1427\n",
      "Epoch [2/2], Step [277/420], Loss: 0.1119\n",
      "Epoch [2/2], Step [278/420], Loss: 0.1966\n",
      "Epoch [2/2], Step [279/420], Loss: 0.2573\n",
      "Epoch [2/2], Step [280/420], Loss: 0.2252\n",
      "Epoch [2/2], Step [281/420], Loss: 0.1270\n",
      "Epoch [2/2], Step [282/420], Loss: 0.1337\n",
      "Epoch [2/2], Step [283/420], Loss: 0.1645\n",
      "Epoch [2/2], Step [284/420], Loss: 0.0861\n",
      "Epoch [2/2], Step [285/420], Loss: 0.1216\n",
      "Epoch [2/2], Step [286/420], Loss: 0.7135\n",
      "Epoch [2/2], Step [287/420], Loss: 0.0255\n",
      "Epoch [2/2], Step [288/420], Loss: 0.2211\n",
      "Epoch [2/2], Step [289/420], Loss: 0.2776\n",
      "Epoch [2/2], Step [290/420], Loss: 0.2831\n",
      "Epoch [2/2], Step [291/420], Loss: 0.0647\n",
      "Epoch [2/2], Step [292/420], Loss: 0.1884\n",
      "Epoch [2/2], Step [293/420], Loss: 0.0154\n",
      "Epoch [2/2], Step [294/420], Loss: 0.0594\n",
      "Epoch [2/2], Step [295/420], Loss: 0.1177\n",
      "Epoch [2/2], Step [296/420], Loss: 0.0925\n",
      "Epoch [2/2], Step [297/420], Loss: 0.2224\n",
      "Epoch [2/2], Step [298/420], Loss: 0.2396\n",
      "Epoch [2/2], Step [299/420], Loss: 0.1668\n",
      "Epoch [2/2], Step [300/420], Loss: 0.1209\n",
      "Epoch [2/2], Step [301/420], Loss: 0.0111\n",
      "Epoch [2/2], Step [302/420], Loss: 0.0920\n",
      "Epoch [2/2], Step [303/420], Loss: 0.3079\n",
      "Epoch [2/2], Step [304/420], Loss: 0.3035\n",
      "Epoch [2/2], Step [305/420], Loss: 0.4040\n",
      "Epoch [2/2], Step [306/420], Loss: 0.0462\n",
      "Epoch [2/2], Step [307/420], Loss: 0.1267\n",
      "Epoch [2/2], Step [308/420], Loss: 0.1818\n",
      "Epoch [2/2], Step [309/420], Loss: 0.0695\n",
      "Epoch [2/2], Step [310/420], Loss: 0.1817\n",
      "Epoch [2/2], Step [311/420], Loss: 0.0545\n",
      "Epoch [2/2], Step [312/420], Loss: 0.2850\n",
      "Epoch [2/2], Step [313/420], Loss: 0.0574\n",
      "Epoch [2/2], Step [314/420], Loss: 0.1542\n",
      "Epoch [2/2], Step [315/420], Loss: 0.1611\n",
      "Epoch [2/2], Step [316/420], Loss: 0.2086\n",
      "Epoch [2/2], Step [317/420], Loss: 0.1806\n",
      "Epoch [2/2], Step [318/420], Loss: 0.3988\n",
      "Epoch [2/2], Step [319/420], Loss: 0.1452\n",
      "Epoch [2/2], Step [320/420], Loss: 0.1203\n",
      "Epoch [2/2], Step [321/420], Loss: 0.4012\n",
      "Epoch [2/2], Step [322/420], Loss: 0.3502\n",
      "Epoch [2/2], Step [323/420], Loss: 0.0985\n",
      "Epoch [2/2], Step [324/420], Loss: 0.1155\n",
      "Epoch [2/2], Step [325/420], Loss: 0.1091\n",
      "Epoch [2/2], Step [326/420], Loss: 0.1553\n",
      "Epoch [2/2], Step [327/420], Loss: 0.0843\n",
      "Epoch [2/2], Step [328/420], Loss: 0.1273\n",
      "Epoch [2/2], Step [329/420], Loss: 0.0796\n",
      "Epoch [2/2], Step [330/420], Loss: 0.0824\n",
      "Epoch [2/2], Step [331/420], Loss: 0.2368\n",
      "Epoch [2/2], Step [332/420], Loss: 0.1286\n",
      "Epoch [2/2], Step [333/420], Loss: 0.1218\n",
      "Epoch [2/2], Step [334/420], Loss: 0.0455\n",
      "Epoch [2/2], Step [335/420], Loss: 0.1650\n",
      "Epoch [2/2], Step [336/420], Loss: 0.3038\n",
      "Epoch [2/2], Step [337/420], Loss: 0.0872\n",
      "Epoch [2/2], Step [338/420], Loss: 0.1090\n",
      "Epoch [2/2], Step [339/420], Loss: 0.0483\n",
      "Epoch [2/2], Step [340/420], Loss: 0.1637\n",
      "Epoch [2/2], Step [341/420], Loss: 0.1793\n",
      "Epoch [2/2], Step [342/420], Loss: 0.0592\n",
      "Epoch [2/2], Step [343/420], Loss: 0.0324\n",
      "Epoch [2/2], Step [344/420], Loss: 0.0414\n",
      "Epoch [2/2], Step [345/420], Loss: 0.0987\n",
      "Epoch [2/2], Step [346/420], Loss: 0.0688\n",
      "Epoch [2/2], Step [347/420], Loss: 0.1329\n",
      "Epoch [2/2], Step [348/420], Loss: 0.1549\n",
      "Epoch [2/2], Step [349/420], Loss: 0.1991\n",
      "Epoch [2/2], Step [350/420], Loss: 0.1775\n",
      "Epoch [2/2], Step [351/420], Loss: 0.0843\n",
      "Epoch [2/2], Step [352/420], Loss: 0.3587\n",
      "Epoch [2/2], Step [353/420], Loss: 0.0650\n",
      "Epoch [2/2], Step [354/420], Loss: 0.1245\n",
      "Epoch [2/2], Step [355/420], Loss: 0.0839\n",
      "Epoch [2/2], Step [356/420], Loss: 0.2398\n",
      "Epoch [2/2], Step [357/420], Loss: 0.3375\n",
      "Epoch [2/2], Step [358/420], Loss: 0.1756\n",
      "Epoch [2/2], Step [359/420], Loss: 0.0164\n",
      "Epoch [2/2], Step [360/420], Loss: 0.1939\n",
      "Epoch [2/2], Step [361/420], Loss: 0.0883\n",
      "Epoch [2/2], Step [362/420], Loss: 0.1026\n",
      "Epoch [2/2], Step [363/420], Loss: 0.3296\n",
      "Epoch [2/2], Step [364/420], Loss: 0.0885\n",
      "Epoch [2/2], Step [365/420], Loss: 0.0458\n",
      "Epoch [2/2], Step [366/420], Loss: 0.0252\n",
      "Epoch [2/2], Step [367/420], Loss: 0.0453\n",
      "Epoch [2/2], Step [368/420], Loss: 0.1497\n",
      "Epoch [2/2], Step [369/420], Loss: 0.0926\n",
      "Epoch [2/2], Step [370/420], Loss: 0.0929\n",
      "Epoch [2/2], Step [371/420], Loss: 0.0920\n",
      "Epoch [2/2], Step [372/420], Loss: 0.1113\n",
      "Epoch [2/2], Step [373/420], Loss: 0.2261\n",
      "Epoch [2/2], Step [374/420], Loss: 0.1427\n",
      "Epoch [2/2], Step [375/420], Loss: 0.1925\n",
      "Epoch [2/2], Step [376/420], Loss: 0.0524\n",
      "Epoch [2/2], Step [377/420], Loss: 0.1719\n",
      "Epoch [2/2], Step [378/420], Loss: 0.1146\n",
      "Epoch [2/2], Step [379/420], Loss: 0.1704\n",
      "Epoch [2/2], Step [380/420], Loss: 0.0768\n",
      "Epoch [2/2], Step [381/420], Loss: 0.0972\n",
      "Epoch [2/2], Step [382/420], Loss: 0.2441\n",
      "Epoch [2/2], Step [383/420], Loss: 0.1501\n",
      "Epoch [2/2], Step [384/420], Loss: 0.1884\n",
      "Epoch [2/2], Step [385/420], Loss: 0.2049\n",
      "Epoch [2/2], Step [386/420], Loss: 0.1296\n",
      "Epoch [2/2], Step [387/420], Loss: 0.1140\n",
      "Epoch [2/2], Step [388/420], Loss: 0.1187\n",
      "Epoch [2/2], Step [389/420], Loss: 0.1233\n",
      "Epoch [2/2], Step [390/420], Loss: 0.1110\n",
      "Epoch [2/2], Step [391/420], Loss: 0.1367\n",
      "Epoch [2/2], Step [392/420], Loss: 0.1294\n",
      "Epoch [2/2], Step [393/420], Loss: 0.0421\n",
      "Epoch [2/2], Step [394/420], Loss: 0.1534\n",
      "Epoch [2/2], Step [395/420], Loss: 0.0042\n",
      "Epoch [2/2], Step [396/420], Loss: 0.0377\n",
      "Epoch [2/2], Step [397/420], Loss: 0.0615\n",
      "Epoch [2/2], Step [398/420], Loss: 0.1760\n",
      "Epoch [2/2], Step [399/420], Loss: 0.0683\n",
      "Epoch [2/2], Step [400/420], Loss: 0.0380\n",
      "Epoch [2/2], Step [401/420], Loss: 0.0370\n",
      "Epoch [2/2], Step [402/420], Loss: 0.1979\n",
      "Epoch [2/2], Step [403/420], Loss: 0.1612\n",
      "Epoch [2/2], Step [404/420], Loss: 0.1764\n",
      "Epoch [2/2], Step [405/420], Loss: 0.0638\n",
      "Epoch [2/2], Step [406/420], Loss: 0.2219\n",
      "Epoch [2/2], Step [407/420], Loss: 0.0124\n",
      "Epoch [2/2], Step [408/420], Loss: 0.1383\n",
      "Epoch [2/2], Step [409/420], Loss: 0.1326\n",
      "Epoch [2/2], Step [410/420], Loss: 0.0262\n",
      "Epoch [2/2], Step [411/420], Loss: 0.1284\n",
      "Epoch [2/2], Step [412/420], Loss: 0.0859\n",
      "Epoch [2/2], Step [413/420], Loss: 0.1923\n",
      "Epoch [2/2], Step [414/420], Loss: 0.0811\n",
      "Epoch [2/2], Step [415/420], Loss: 0.1318\n",
      "Epoch [2/2], Step [416/420], Loss: 0.1102\n",
      "Epoch [2/2], Step [417/420], Loss: 0.0574\n",
      "Epoch [2/2], Step [418/420], Loss: 0.0122\n",
      "Epoch [2/2], Step [419/420], Loss: 0.2068\n",
      "Epoch [2/2], Step [420/420], Loss: 0.0190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith torch.no_grad():\\n    n_correct = 0\\n    n_samples = 0\\n    for samples, labels in test_loader:\\n        samples = samples.reshape(-1, 28*28).to(device)\\n        labels = labels.reshape(-1,1).to(device)\\n        outputs = model(samples)\\n        # max returns (value ,index)\\n        _, predicted = torch.max(outputs.data, 1)\\n        n_samples += labels.size(0)\\n        n_correct += (predicted == labels).sum().item()\\n\\n    acc = 100.0 * n_correct / n_samples\\n    print(f'Accuracy of the network on the 10000 test images: {acc} %')      \\n\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        samples = samples.to(device)\n",
    "        outputs = model(samples)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print (f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency\n",
    "model.eval()\n",
    "test_csv = pd.read_csv(TEST_PATH)\n",
    "N = len(test_csv)\n",
    "predict_df = pd.DataFrame(columns=['ImageId','Label'])\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i in range(N):\n",
    "        #print(i/N)\n",
    "        image = torch.tensor(test_csv.iloc[i, :]).view(28,28).to(torch.float32).to(device)\n",
    "        #plt.figure()\n",
    "        #plt.imshow(image)\n",
    "        #plt.show()\n",
    "        output = torch.argmax(model(image)).item()\n",
    "        #print(output)\n",
    "        predict_df = predict_df.append({'ImageId': i+1, 'Label': output}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf36167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.to_csv('data/my_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
